{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c9a6b7c",
   "metadata": {},
   "source": [
    "## Preprocess dataset for BERT pretraining\n",
    "\n",
    "1. Prepare the dataset\n",
    "2. Train a Tokenizer\n",
    "3. Preprocess the dataset\n",
    "\n",
    "Compute resource: AWS EC2 c5n.18xlarge instance\n",
    "\n",
    "Reference: https://github.com/philschmid/deep-learning-habana-huggingface/blob/master/pre-training/pre-training-bert.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eaec41",
   "metadata": {},
   "source": [
    "## 1. Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7554e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62882572445347eca26bb5870a109f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# log into Hugging Face Hub\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67dd2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "user_id = HfApi().whoami()[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9930564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delmeng'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe026886",
   "metadata": {},
   "source": [
    "The original BERT was pretrained on Wikipedia and BookCorpus dataset. Both datasets are available on the Hugging Face Hub and can be loaded with `datasets`.\n",
    "\n",
    "https://huggingface.co/datasets/wikipedia  \n",
    "https://huggingface.co/datasets/bookcorpus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b787b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4436ea51ffa47a192fb001dc6a73c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb4167dbce249f38ae15b4890b9c737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d26746f29024c7abda56723604c5d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf82979bf80d433baf48b8ef6f42b4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/74004228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62b04a5da744d48bd75941cb30ff889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/35.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c7594da384431997530585105621d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/30.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca01cbada9514a18b8f4973d298a1630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4aa6aedc7743e39f9d4d549d7cff34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/15.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5746410e84a74ea3b6e2b79b58fafc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/20.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# note that this step is slow\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "wikipedia = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "wikipedia = wikipedia.remove_columns([col for col in wikipedia.column_names if col != \"text\"])  # only keep the 'text' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bookcorpus.features.type == wikipedia.features.type\n",
    "raw_datasets = concatenate_datasets([bookcorpus, wikipedia])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92169acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6698463e",
   "metadata": {},
   "source": [
    "## 2. Train a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3986f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# repositor id for saving the tokenizer\n",
    "tokenizer_id=\"bert-base-uncased-2023-pc-p4d\"\n",
    "\n",
    "# create a python generator to dynamically load the data\n",
    "def batch_iterator(batch_size=10000):\n",
    "    for i in tqdm(range(0, len(raw_datasets), batch_size)):\n",
    "        yield raw_datasets[i : i + batch_size][\"text\"]\n",
    "\n",
    "# create a tokenizer from existing one to re-use special tokens\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d038fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=32_000)\n",
    "bert_tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5cd809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the tokenizer to Hugging Face Hub for later training our model\n",
    "# you need to be logged into push the tokenizer\n",
    "bert_tokenizer.push_to_hub(tokenizer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71468d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "372c0587",
   "metadata": {},
   "source": [
    "## 3. Preprocess the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6283c28c",
   "metadata": {},
   "source": [
    "Before we can get started with training our model, the last step is to pre-process/tokenize our dataset. We will use our trained tokenizer to tokenize our dataset and then push it to hub to load it easily later in our training. The tokenization process is also kept pretty simple, if documents are longer than 512 tokens those are truncated and not split into several documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cfbba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import multiprocessing\n",
    "\n",
    "# load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(f\"{user_id}/{tokenizer_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")\n",
    "\n",
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "       examples[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=tokenizer.model_max_length\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# preprocess dataset\n",
    "tokenized_datasets = raw_datasets.map(group_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "tokenized_datasets.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2315f7",
   "metadata": {},
   "source": [
    "As data processing function will we concatenate all texts from our dataset and generate chunks of tokenizer.model_max_length (512)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2531d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= tokenizer.model_max_length:\n",
    "        total_length = (total_length // tokenizer.model_max_length) * tokenizer.model_max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + tokenizer.model_max_length] for i in range(0, total_length, tokenizer.model_max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(group_texts, batched=True, num_proc=num_proc)\n",
    "# shuffle dataset\n",
    "tokenized_datasets = tokenized_datasets.shuffle(seed=34)\n",
    "\n",
    "print(f\"the dataset contains in total {len(tokenized_datasets)*tokenizer.model_max_length} tokens\")\n",
    "# the dataset contains in total 3417216000 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d50b22",
   "metadata": {},
   "source": [
    "The last step before we can start with out training is to push our prepared dataset to the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ad508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push dataset to hugging face\n",
    "dataset_id=f\"{user_id}/processed_bert_dataset\"\n",
    "tokenized_datasets.push_to_hub(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d0128b",
   "metadata": {},
   "source": [
    "Processed dataset: https://huggingface.co/datasets/delmeng/processed_bert_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8e693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
